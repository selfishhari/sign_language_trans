{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize\n",
    "def resize_vid(fname, vid_folder):\n",
    "    \n",
    "    try:\n",
    "\n",
    "\n",
    "        from pathlib import Path\n",
    "\n",
    "        re_dir = os.path.join(os.path.abspath(vid_folder), \"proc_vids\\(copy\\)\")\n",
    "\n",
    "        Path(re_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        re_fname = os.path.join(os.path.abspath(re_dir), fname)\n",
    "\n",
    "        fname_abs = os.path.join(os.path.abspath(vid_folder), fname)\n",
    "\n",
    "        #print(\"ffmpeg -i {} -vf scale=98:-2 {}\".format(fname_abs, re_fname))\n",
    "        os.system(\"ffmpeg -i {} -vf scale=96:96 {}\".format(fname_abs, re_fname))\n",
    "    except:\n",
    "        print(\"error\", fname)\n",
    "    \n",
    "    return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#resize\n",
    "def resize_vid(fname, vid_folder):\n",
    "    \n",
    "    try:\n",
    "\n",
    "\n",
    "        from pathlib import Path\n",
    "\n",
    "        re_dir = os.path.join(os.path.abspath(vid_folder), \"proc_vids\")\n",
    "\n",
    "        Path(re_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        re_fname = os.path.join(os.path.abspath(re_dir), fname)\n",
    "\n",
    "        #fname_abs = os.path.join(os.path.abspath(vid_folder), fname)\n",
    "        \n",
    "        fname_abs = os.path.join(os.path.abspath(re_dir), fname)\n",
    "\n",
    "        #print(\"ffmpeg -i {} -vf scale=98:-2 {}\".format(fname_abs, re_fname))\n",
    "        os.system(\"ffmpeg -i {} -vf scale=64:64 -y {}\".format(fname_abs, re_fname))\n",
    "    except:\n",
    "        print(\"error\", fname)\n",
    "    \n",
    "    return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, math, torch, pandas as pd, numpy as np, glob, skvideo.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0,\"../\")\n",
    "\n",
    "\n",
    "from src.mocogan_model import Discriminator_I, Discriminator_V, Generator_I, GRU\n",
    "\n",
    "from importlib import reload\n",
    "import src\n",
    "reload(src.mocogan_model)\n",
    "\n",
    "from src.mocogan_model import Discriminator_I, Discriminator_V, Generator_I, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda       = -1#cpu\n",
    "ngpu       = 1\n",
    "batch_size = 16\n",
    "n_iter     = 120000\n",
    "pre_train  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "if cuda == True:\n",
    "    torch.cuda.set_device(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.abspath(os.path.join(\"..\",\"data\"))\n",
    "\n",
    "VID_DIR = os.path.join(DATA_DIR, 'raw', \"UCF101\")\n",
    "\n",
    "PROC_VID_DIR = os.path.join(VID_DIR, 'proc_vids(copy)')\n",
    "\n",
    "MODEL_DIR = os.path.join(DATA_DIR, 'models')\n",
    "\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, 'outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vid(fname, dir_name):\n",
    "    \n",
    "    abs_fname = os.path.join(dir_name,fname)\n",
    "    \n",
    "    #print(fname)\n",
    "    \n",
    "    return skvideo.io.vread(abs_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(PROC_VID_DIR)\n",
    "\n",
    "videos = [ read_vid(file, PROC_VID_DIR) for file in files[:10] ]\n",
    "\n",
    "# transpose each video to (nc, n_frames, img_size, img_size), and divide by 255\n",
    "videos = [ video.transpose(3, 0, 1, 2) / 255.0 for video in videos ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "resized_fnames = [resize_vid(x, VID_DIR) for x in files[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_videos = len(videos)\n",
    "\n",
    "T = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for true video\n",
    "def trim(video):\n",
    "    start = np.random.randint(0, video.shape[1] - (T+1))\n",
    "    end = start + T\n",
    "    #print(start, end)\n",
    "    return video[:, start:end, :, :]\n",
    "\n",
    "def trim_noise(noise):\n",
    "    start = np.random.randint(0, noise.size(1) - (T+1))\n",
    "    end = start + T\n",
    "    return noise[:, start:end, :, :, :]\n",
    "\n",
    "def random_choice():\n",
    "    X = []\n",
    "    for _ in range(batch_size):\n",
    "        video = videos[np.random.randint(0, n_videos-1)]\n",
    "        video = torch.Tensor(trim(video))\n",
    "        X.append(video)\n",
    "    #print(X.shape)\n",
    "    X = torch.stack(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video length distribution\n",
    "video_lengths = [video.shape[1] for video in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' set models '''\n",
    "\n",
    "img_size = 96\n",
    "\n",
    "nc = 3\n",
    "\n",
    "ndf = 64 # from dcgan\n",
    "\n",
    "ngf = 64\n",
    "\n",
    "d_E = 10\n",
    "\n",
    "hidden_size = 100 # guess\n",
    "\n",
    "d_C = 50\n",
    "\n",
    "d_M = d_E\n",
    "\n",
    "nz  = d_C + d_M\n",
    "\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/mocogan_model.py:145: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(params)\n",
      "../src/mocogan_model.py:155: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(params, 0)\n"
     ]
    }
   ],
   "source": [
    "dis_i = Discriminator_I(nc, ndf, ngpu=ngpu)\n",
    "\n",
    "dis_v = Discriminator_V(nc, ndf, T=T, ngpu=ngpu)\n",
    "\n",
    "gen_i = Generator_I(nc, ngf, nz, ngpu=ngpu)\n",
    "\n",
    "gru = GRU(d_E, hidden_size, gpu=cuda)\n",
    "\n",
    "gru.initWeight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11042561\n",
      "2775808\n",
      "3863424\n",
      "34610\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in dis_v.parameters() if p.requires_grad))\n",
    "\n",
    "print(sum(p.numel() for p in dis_i.parameters() if p.requires_grad))\n",
    "\n",
    "print(sum(p.numel() for p in gen_i.parameters() if p.requires_grad))\n",
    "\n",
    "print(sum(p.numel() for p in gru.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' prepare for train '''\n",
    "\n",
    "label = torch.FloatTensor()\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    d = math.floor(s / ((60**2)*24))\n",
    "    h = math.floor(s / (60**2)) - d*24\n",
    "    m = math.floor(s / 60) - h*60 - d*24*60\n",
    "    s = s - m*60 - h*(60**2) - d*24*(60**2)\n",
    "    return '%dd %dh %dm %ds' % (d, h, m, s)\n",
    "\n",
    "\n",
    "\n",
    "def checkpoint(model, optimizer, epoch):\n",
    "    \n",
    "    filename = os.path.join(MODEL_DIR, '%s_epoch-%d' % (model.__class__.__name__, epoch))\n",
    "    \n",
    "    torch.save(model.state_dict(), filename + '.model')\n",
    "    \n",
    "    torch.save(optimizer.state_dict(), filename + '.state')\n",
    "\n",
    "def save_video(fake_video, epoch):\n",
    "    \n",
    "    outputdata = fake_video * 255\n",
    "    \n",
    "    outputdata = outputdata.astype(np.uint8)\n",
    "    \n",
    "    gen_vid_dir = os.path.join(OUTPUT_DIR, 'generated_videos')\n",
    "    \n",
    "    Path(gen_vid_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(gen_vid_dir, 'fakeVideo_epoch-%d.mp4' % epoch)\n",
    "    \n",
    "    skvideo.io.vwrite(file_path, outputdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' adjust to cuda '''\n",
    "\n",
    "if cuda == True:\n",
    "    \n",
    "    dis_i.cuda()\n",
    "    \n",
    "    dis_v.cuda()\n",
    "    \n",
    "    gen_i.cuda()\n",
    "    \n",
    "    gru.cuda()\n",
    "    \n",
    "    criterion.cuda()\n",
    "    \n",
    "    label = label.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizer\n",
    "lr = 0.0002\n",
    "\n",
    "betas=(0.5, 0.999)\n",
    "\n",
    "optim_Di  = optim.Adam(dis_i.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "optim_Dv  = optim.Adam(dis_v.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "optim_Gi  = optim.Adam(gen_i.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "optim_GRU = optim.Adam(gru.parameters(),   lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' use pre-trained models '''\n",
    "\n",
    "if pre_train == True:\n",
    "    dis_i.load_state_dict(torch.load(trained_path + '/Discriminator_I.model'))\n",
    "    dis_v.load_state_dict(torch.load(trained_path + '/Discriminator_V.model'))\n",
    "    gen_i.load_state_dict(torch.load(trained_path + '/Generator_I.model'))\n",
    "    gru.load_state_dict(torch.load(trained_path + '/GRU.model'))\n",
    "    optim_Di.load_state_dict(torch.load(trained_path + '/Discriminator_I.state'))\n",
    "    optim_Dv.load_state_dict(torch.load(trained_path + '/Discriminator_V.state'))\n",
    "    optim_Gi.load_state_dict(torch.load(trained_path + '/Generator_I.state'))\n",
    "    optim_GRU.load_state_dict(torch.load(trained_path + '/GRU.state'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' calc grad of models '''\n",
    "\n",
    "def bp_i(inputs, y, retain=False):\n",
    "    \n",
    "    label.resize_(inputs.size(0)).fill_(y)\n",
    "    \n",
    "    labelv = Variable(label)\n",
    "    \n",
    "    outputs = dis_i(inputs)\n",
    "    \n",
    "    err = criterion(outputs, labelv)\n",
    "    \n",
    "    err.backward(retain_graph=retain)\n",
    "    \n",
    "    return err.data, outputs.data.mean()\n",
    "\n",
    "def bp_v(inputs, y, retain=False):\n",
    "    \n",
    "    label.resize_(inputs.size(0)).fill_(y)\n",
    "    \n",
    "    labelv = Variable(label)\n",
    "    \n",
    "    outputs = dis_v(inputs)\n",
    "    \n",
    "    err = criterion(outputs, labelv)\n",
    "    \n",
    "    err.backward(retain_graph=retain)\n",
    "    \n",
    "    return err.data, outputs.data.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' gen input noise for fake video '''\n",
    "\n",
    "def gen_z(n_frames):\n",
    "    \n",
    "    z_C = Variable(torch.randn(batch_size, d_C))\n",
    "    \n",
    "    #  repeat z_C to (batch_size, n_frames, d_C)\n",
    "    \n",
    "    z_C = z_C.unsqueeze(1).repeat(1, n_frames, 1)\n",
    "    \n",
    "    #print(z_C.shape)\n",
    "    \n",
    "    eps = Variable(torch.randn(batch_size, d_E))\n",
    "    \n",
    "    #print(eps.shape)\n",
    "    \n",
    "    if cuda == True:\n",
    "        z_C, eps = z_C.cuda(), eps.cuda()\n",
    "\n",
    "    gru.initHidden(batch_size)\n",
    "    \n",
    "    # notice that 1st dim of gru outputs is seq_len, 2nd is batch_size\n",
    "    \n",
    "    z_M = gru(eps, n_frames).transpose(1, 0)\n",
    "    \n",
    "    #print(z_M.shape)\n",
    "    \n",
    "    z = torch.cat((z_M, z_C), 2)  # z.size() => (batch_size, n_frames, nz)\n",
    "    \n",
    "    #print(z.shape)\n",
    "    \n",
    "    return z.view(batch_size, n_frames, nz, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 60, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = trim_noise(gen_z(18))\n",
    "\n",
    "s.contiguous().view(batch_size*T, nz, 1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dis_v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-20e3bc2fb317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdis_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dis_v' is not defined"
     ]
    }
   ],
   "source": [
    "dis_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 16\n",
      "25 41\n",
      "42 58\n",
      "28 44\n",
      "68 84\n",
      "122 138\n",
      "135 151\n",
      "61 77\n",
      "5 21\n",
      "8 24\n",
      "463 479\n",
      "139 155\n",
      "39 55\n",
      "45 61\n",
      "74 90\n",
      "177 193\n",
      "51 67\n",
      "58 74\n",
      "55 71\n",
      "173 189\n",
      "65 81\n",
      "53 69\n",
      "94 110\n",
      "80 96\n",
      "124 140\n",
      "58 74\n",
      "73 89\n",
      "13 29\n",
      "38 54\n",
      "136 152\n",
      "135 151\n",
      "22 38\n",
      "209 225\n",
      "468 484\n",
      "66 82\n",
      "144 160\n",
      "114 130\n",
      "24 40\n",
      "100 116\n",
      "87 103\n",
      "54 70\n",
      "20 36\n",
      "101 117\n",
      "151 167\n",
      "29 45\n",
      "115 131\n",
      "119 135\n",
      "86 102\n",
      "83 99\n",
      "148 164\n",
      "42 58\n",
      "20 36\n",
      "68 84\n",
      "47 63\n",
      "43 59\n",
      "31 47\n",
      "9 25\n",
      "91 107\n",
      "83 99\n",
      "274 290\n",
      "43 59\n",
      "12 28\n",
      "1 17\n",
      "39 55\n",
      "134 150\n",
      "98 114\n",
      "28 44\n",
      "26 42\n",
      "248 264\n",
      "135 151\n",
      "72 88\n",
      "135 151\n",
      "21 37\n",
      "108 124\n",
      "38 54\n",
      "71 87\n",
      "0 16\n",
      "91 107\n",
      "7 23\n",
      "101 117\n",
      "[5http://localhost:8888/notebooks/notebooks/ucf101_mocogan.ipynb#/120000] (0d 0h 2m 37s) Loss_Di: 0.4000 Loss_Dv: 0.4405 Loss_Gi: 5.7454 Loss_Gv: 10.8776 Di_real_mean 0.8719 Di_fake_mean 0.0339 Dv_real_mean 0.9767 Dv_fake_mean 0.0000\n",
      "104 120\n",
      "38 54\n",
      "74 90\n",
      "123 139\n",
      "26 42\n",
      "32 48\n",
      "97 113\n",
      "59 75\n",
      "178 194\n",
      "132 148\n",
      "93 109\n",
      "145 161\n",
      "148 164\n",
      "119 135\n",
      "105 121\n",
      "4 20\n",
      "35 51\n",
      "90 106\n",
      "26 42\n",
      "98 114\n",
      "51 67\n",
      "208 224\n",
      "188 204\n",
      "104 120\n",
      "10 26\n",
      "141 157\n",
      "94 110\n",
      "173 189\n",
      "15 31\n",
      "166 182\n",
      "171 187\n",
      "7 23\n",
      "78 94\n",
      "100 116\n",
      "221 237\n",
      "46 62\n",
      "114 130\n",
      "12 28\n",
      "233 249\n",
      "2 18\n",
      "125 141\n",
      "27 43\n",
      "120 136\n",
      "192 208\n",
      "88 104\n",
      "207 223\n",
      "60 76\n",
      "171 187\n",
      "28 44\n",
      "66 82\n",
      "92 108\n",
      "109 125\n",
      "54 70\n",
      "21 37\n",
      "81 97\n",
      "136 152\n",
      "21 37\n",
      "144 160\n",
      "119 135\n",
      "127 143\n",
      "5 21\n",
      "140 156\n",
      "74 90\n",
      "78 94\n",
      "187 203\n",
      "138 154\n",
      "89 105\n",
      "78 94\n",
      "69 85\n",
      "4 20\n",
      "69 85\n",
      "30 46\n",
      "3 19\n",
      "168 184\n",
      "58 74\n",
      "55 71\n",
      "33 49\n",
      "97 113\n",
      "7 23\n",
      "46 62\n",
      "[10http://localhost:8888/notebooks/notebooks/ucf101_mocogan.ipynb#/120000] (0d 0h 4m 53s) Loss_Di: 0.8616 Loss_Dv: 0.4280 Loss_Gi: 11.8177 Loss_Gv: 13.2025 Di_real_mean 0.8624 Di_fake_mean 0.3269 Dv_real_mean 0.7934 Dv_fake_mean 0.0000\n",
      "54 70\n",
      "115 131\n",
      "25 41\n",
      "52 68\n",
      "76 92\n",
      "18 34\n",
      "7 23\n",
      "48 64\n",
      "293 309\n",
      "109 125\n",
      "51 67\n",
      "71 87\n",
      "49 65\n",
      "3 19\n",
      "65 81\n",
      "42 58\n",
      "48 64\n",
      "102 118\n",
      "2 18\n",
      "100 116\n",
      "68 84\n",
      "12 28\n",
      "165 181\n",
      "289 305\n",
      "99 115\n",
      "50 66\n",
      "18 34\n",
      "72 88\n",
      "153 169\n",
      "28 44\n",
      "18 34\n",
      "19 35\n",
      "6 22\n",
      "99 115\n",
      "198 214\n",
      "162 178\n",
      "114 130\n",
      "118 134\n",
      "100 116\n",
      "225 241\n",
      "15 31\n",
      "151 167\n",
      "5 21\n",
      "1 17\n",
      "6 22\n",
      "102 118\n",
      "75 91\n",
      "126 142\n",
      "157 173\n",
      "115 131\n",
      "133 149\n",
      "193 209\n",
      "62 78\n",
      "47 63\n",
      "149 165\n",
      "89 105\n",
      "92 108\n",
      "6 22\n",
      "40 56\n",
      "158 174\n",
      "15 31\n",
      "37 53\n",
      "59 75\n",
      "80 96\n",
      "64 80\n",
      "26 42\n",
      "133 149\n",
      "83 99\n",
      "92 108\n",
      "154 170\n",
      "140 156\n",
      "52 68\n",
      "29 45\n",
      "87 103\n",
      "330 346\n",
      "61 77\n",
      "16 32\n",
      "84 100\n",
      "62 78\n",
      "48 64\n",
      "[15http://localhost:8888/notebooks/notebooks/ucf101_mocogan.ipynb#/120000] (0d 0h 7m 25s) Loss_Di: 1.4047 Loss_Dv: 0.4066 Loss_Gi: 9.7297 Loss_Gv: 6.5837 Di_real_mean 0.3111 Di_fake_mean 0.0001 Dv_real_mean 0.8021 Dv_fake_mean 0.0014\n",
      "79 95\n",
      "33 49\n",
      "16 32\n",
      "318 334\n",
      "91 107\n",
      "22 38\n",
      "158 174\n",
      "27 43\n",
      "114 130\n",
      "215 231\n",
      "103 119\n",
      "90 106\n",
      "54 70\n",
      "35 51\n",
      "365 381\n",
      "112 128\n",
      "107 123\n",
      "96 112\n",
      "69 85\n",
      "29 45\n",
      "80 96\n",
      "150 166\n",
      "19 35\n",
      "20 36\n",
      "67 83\n",
      "229 245\n",
      "41 57\n",
      "40 56\n",
      "29 45\n",
      "94 110\n",
      "126 142\n",
      "16 32\n",
      "23 39\n",
      "128 144\n",
      "37 53\n",
      "26 42\n",
      "89 105\n",
      "216 232\n",
      "101 117\n",
      "145 161\n",
      "43 59\n",
      "118 134\n",
      "22 38\n",
      "11 27\n",
      "135 151\n",
      "48 64\n",
      "239 255\n",
      "96 112\n",
      "8 24\n",
      "5 21\n",
      "38 54\n",
      "117 133\n",
      "133 149\n",
      "38 54\n",
      "88 104\n",
      "79 95\n",
      "114 130\n",
      "132 148\n",
      "24 40\n",
      "130 146\n",
      "131 147\n",
      "11 27\n",
      "81 97\n",
      "175 191\n",
      "145 161\n",
      "85 101\n",
      "36 52\n",
      "98 114\n",
      "226 242\n",
      "147 163\n",
      "225 241\n",
      "97 113\n",
      "45 61\n",
      "89 105\n",
      "45 61\n",
      "51 67\n",
      "151 167\n",
      "3 19\n",
      "46 62\n",
      "2 18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e24004dba6bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0merr_Dv_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDv_real_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbp_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_videos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0merr_Dv_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDv_fake_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbp_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_videos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0merr_Dv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_Dv_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merr_Dv_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-e77d90446c5b>\u001b[0m in \u001b[0;36mbp_v\u001b[0;34m(inputs, y, retain)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sign/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sign/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' train models '''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, n_iter+1):\n",
    "    \n",
    "    ''' prepare real images '''\n",
    "    # real_videos.size() => (batch_size, nc, T, img_size, img_size)\n",
    "    \n",
    "    real_videos = random_choice()\n",
    "    \n",
    "    if cuda == True:\n",
    "        \n",
    "        real_videos = real_videos.cuda()\n",
    "        \n",
    "    real_videos = Variable(real_videos)\n",
    "    \n",
    "    real_img = real_videos[:, :, np.random.randint(0, T), :, :]\n",
    "\n",
    "    ''' prepare fake images '''\n",
    "    # note that n_frames is sampled from video length distribution\n",
    "    \n",
    "    n_frames = video_lengths[np.random.randint(0, n_videos)]\n",
    "    \n",
    "    Z = gen_z(n_frames)  # Z.size() => (batch_size, n_frames, nz, 1, 1)\n",
    "    \n",
    "    # trim => (batch_size, T, nz, 1, 1)\n",
    "    \n",
    "    Z = trim_noise(Z)\n",
    "    # generate videos\n",
    "    \n",
    "    Z = Z.contiguous().view(batch_size*T, nz, 1, 1)\n",
    "    # concatenate samples and frames into samples*frame samples of vectors.\n",
    "    #Each vector will now have a dim of (d_M+d_C)*1*1\n",
    "    \n",
    "    fake_videos = gen_i(Z)\n",
    "    \n",
    "    fake_videos = fake_videos.view(batch_size, T, nc, img_size, img_size)\n",
    "    \n",
    "    # transpose => (batch_size, nc, T, img_size, img_size)\n",
    "    \n",
    "    fake_videos = fake_videos.transpose(2, 1)\n",
    "    \n",
    "    # img sampling\n",
    "    fake_img = fake_videos[:, :, np.random.randint(0, T), :, :]\n",
    "\n",
    "    ''' train discriminators '''\n",
    "    # video\n",
    "    dis_v.zero_grad()\n",
    "    \n",
    "    err_Dv_real, Dv_real_mean = bp_v(real_videos, 0.9)\n",
    "    \n",
    "    err_Dv_fake, Dv_fake_mean = bp_v(fake_videos.detach(), 0)\n",
    "    \n",
    "    err_Dv = err_Dv_real + err_Dv_fake\n",
    "    \n",
    "    optim_Dv.step()\n",
    "    \n",
    "    # image\n",
    "    dis_i.zero_grad()\n",
    "    \n",
    "    err_Di_real, Di_real_mean = bp_i(real_img, 0.9)\n",
    "    \n",
    "    err_Di_fake, Di_fake_mean = bp_i(fake_img.detach(), 0)\n",
    "    \n",
    "    err_Di = err_Di_real + err_Di_fake\n",
    "    \n",
    "    optim_Di.step()\n",
    "\n",
    "\n",
    "    ''' train generators '''\n",
    "    \n",
    "    gen_i.zero_grad()\n",
    "    \n",
    "    gru.zero_grad()\n",
    "    \n",
    "    # video. notice retain=True for back prop twice\n",
    "    err_Gv, _ = bp_v(fake_videos, 0.9, retain=True)\n",
    "    # images\n",
    "    err_Gi, _ = bp_i(fake_img, 0.9)\n",
    "    \n",
    "    optim_Gi.step()\n",
    "    \n",
    "    optim_GRU.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        \n",
    "        print('[%d%d] (%s) Loss_Di: %.4f Loss_Dv: %.4f Loss_Gi: %.4f Loss_Gv: %.4f Di_real_mean %.4f Di_fake_mean %.4f Dv_real_mean %.4f Dv_fake_mean %.4f'\n",
    "              % (epoch, n_iter, timeSince(start_time), err_Di, err_Dv, err_Gi, err_Gv, Di_real_mean, Di_fake_mean, Dv_real_mean, Dv_fake_mean))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        \n",
    "        save_video(fake_videos[0].data.cpu().numpy().transpose(1, 2, 3, 0), epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint(dis_i, optim_Di, epoch)\n",
    "        checkpoint(dis_v, optim_Dv, epoch)\n",
    "        checkpoint(gen_i, optim_Gi, epoch)\n",
    "        checkpoint(gru,   optim_GRU, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-17-c2de0ea9a299>\u001b[0m(29)\u001b[0;36mbp_v\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     25 \u001b[0;31m    \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     26 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m    \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 29 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> err.data\n",
      "tensor(0.6603)\n",
      "ipdb> err.data[0]\n",
      "*** IndexError: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "ipdb> outputs.data\n",
      "tensor([0.5278, 0.6354, 0.6683, 0.5604, 0.5503, 0.5802, 0.5894, 0.4852, 0.5013,\n",
      "        0.4791, 0.4914, 0.4683, 0.3157, 0.4077, 0.6962, 0.5626])\n",
      "ipdb> outputs.data.mean()\n",
      "tensor(0.5325)\n",
      "ipdb> s\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = trim_noise(gen_z(18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
